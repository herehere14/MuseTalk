This is the **Definitive Technical Manual and Documentation** for the **MuseTalk Warm-Server Edition (v1.5 Custom)**. This document serves as a comprehensive guide for developers, researchers, and system architects aiming to deploy real-time digital humans with sub-second latency.

---

Please cd MuseTalk/MuseTalk to start the following commands and install requirements.txt

THe requirements.txt has the latest working dependencies to run this project. However, you also need to download some dependencies through installing github repo and weights. --} use LLM for this please

(you may spend a long time to download and update dependencies)

Also, the mouth movement is still abit unrealistic even after applying multiple algorithms such as Kalman and one-euro algorithm to reduce noise. 



# MuseTalk: Warm-Server Edition (v1.5)

**The Definitive Architecture Guide & Usage Manual**

**Version:** 1.5.0-Warm-Production
**Author:** Customized for Real-Time Digital Human Interaction
**Architecture:** Persistent Latent Inference with Continuous Session Management

---

## ğŸ“š **Table of Contents**

1. **Executive Summary & Core Philosophy**
2. **System Architecture Deep Dive**
* The "Cold Start" Problem
* The "Warm Server" Solution
* The Continuous Session Pipeline


3. **Hardware & Software Prerequisites**
4. **Installation & Environment Setup**
5. **Phase 1: The Force-Prep Protocol (Caching)**
* Theory of Operation
* Execution Guide


6. **Phase 2: High-Fidelity Enhancement**
* GFPGAN Integration
* The Enhancement Workflow


7. **Phase 3: The Warm Server (Runtime)**
* API Endpoints
* Under the Hood: `warm_api.py`
* Stabilization Algorithms (One-Euro, Affine)


8. **Phase 4: The Full-Stack Web Application**
* Backend Job System
* Frontend Polling Architecture


9. **Configuration Reference**
10. **Troubleshooting & Optimization**

---

## 1. Executive Summary & Core Philosophy

Standard implementation of Generative AI video models, specifically **MuseTalk**, suffers from a critical bottleneck known as "Cold Start Latency." In a naive implementation, a request to generate a speaking video triggers a cascade of heavy operations: loading 4GB+ of PyTorch weights into VRAM, instantiating the VAE (Variational Autoencoder) and UNet, reading the source video from disk, running face detection algorithms (like S3FD) on every frame, and finally encoding those frames into latent tensors.

For a 5-second response, this "overhead" can take 10-15 seconds before generation even begins. This renders real-time conversation impossible.

**The MuseTalk Warm-Server Edition** fundamentally re-architects this process into a persistent, stateful service. By decoupling the "Preparation Phase" from the "Inference Phase," we achieve:

1. **Sub-Second Time-to-First-Frame (TTFF):** The model waits in VRAM ("warm"), ready to accept audio tensors instantly.
2. **Zero-Shot Face Detection:** Face coordinates and background latents are pre-calculated and cached (`force_prep.py`), eliminating computer vision overhead during the live chat.
3. **Continuous Conversation:** Instead of generating isolated video files, the server intelligently stitches new sentences into a growing, seamless video file using FFmpeg concat-demuxing (`server_fast.py`).

---

## 2. System Architecture Deep Dive

### The "Cold Start" Problem

In standard inference scripts (like the original `inference.py`), the lifecycle of a request is:

1. `import torch` (2s)
2. `Load UNet/VAE` (4s)
3. `Face Detection` (100ms per frame)
4. `VAE Encoding` (50ms per frame)
5. `UNet Inference` (Fast)
6. `VAE Decoding` (Fast)
7. `Cleanup`

For a 100-frame video (4 seconds), steps 3 and 4 alone add 15 seconds of latency on a mid-range GPU.

Also, when it first render AI generated mouth on top of the video uploaded, the rendering will take up to 10 mins on a low range GPU. But after rendering, we are able to 

### The "Warm Server" Solution

The **Warm Server** (`server_fast.py` + `warm_api.py`) changes the lifecycle to:

1. **Server Start:** Load all models and caches (Done once).
2. **Idle State:** Consume ~6GB VRAM, waiting for requests.
3. **Request Received:**
* Audio â†’ Whisper Feature Extractor (100ms)
* UNet Inference (Direct on GPU)
* VAE Decode & Blend
* **Total Latency:** ~0.8s for start of generation.



### The Continuous Session Pipeline

To simulate a real video call, we cannot just return "video_1.mp4", then "video_2.mp4". We need a single, growing stream.

* **The Session File:** `results/full_session.mp4`
* **The Stitcher:** When a new clip is generated, `server_fast.py` generates a temporary text file list (`concat_list.txt`) containing the absolute paths of the *existing session* and the *new clip*.
* **FFmpeg Concat:** It executes `ffmpeg -f concat -c copy ...`. The `-c copy` flag is crucial; it performs a **bitstream copy**, meaning it does not re-encode frames. It simply updates the container metadata. This allows stitching hours of video in milliseconds.

---

## 3. Hardware & Software Prerequisites

### Hardware

* **GPU:** NVIDIA RTX 3060 (12GB) or higher recommended.
* *Minimum:* 8GB VRAM (e.g., RTX 2070). You may need to reduce batch sizes.
* *Ideal:* RTX 4090 (24GB) allows for batch sizes of 32+ and 4K caching.


* **Storage:** NVMe SSD. Loading large `.pt` latent caches from a spinning HDD will introduce stutter.
* **RAM:** 32GB System RAM.

### Software

* **OS:** Linux (Ubuntu 20.04/22.04) or Windows 10/11 (via PowerShell or CMD).
* **Python:** 3.10.x (Strict dependency).
* **CUDA:** 11.8 or 12.1.
* **FFmpeg:** **CRITICAL**. Must be installed and accessible via system PATH.
* Verify by running `ffmpeg -version` in your terminal.



---

## 4. Installation & Environment Setup

### Step 1: Clone and Environment

```bash
# Clone the repository
git clone https://github.com/TMElyralab/MuseTalk.git
cd MuseTalk/MuseTalk

# create and activate environment
python3 venv venv
source venv/bin/activate

# Install PyTorch (Ensure CUDA compatibility)
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118

```

### Step 2: Install Core Dependencies

Use the provided `requirements.txt`.


```bash
pip install -r requirements.txt
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.1"
mim install "mmdet>=3.1.0"
mim install "mmpose>=1.1.0"

```

*Note: We also need `gfpgan` for the enhancement module.*

```bash
pip install gfpgan basicsr

```

### Step 3: Model Weight Acquisition

You must organize your `models/` directory exactly as follows. Use the `download_weights.sh` script if on Linux, or manually place files for Windows. (Ask AI for this since the dependencies are usually changed frequently)

**Directory Structure:**

```text
./models/
â”œâ”€â”€ musetalkV15/       # The V1.5 UNet Checkpoint
â”‚   â”œâ”€â”€ musetalk.json
â”‚   â””â”€â”€ unet.pth
â”œâ”€â”€ sd-vae/            # Stable Diffusion VAE (ft-mse)
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ diffusion_pytorch_model.bin
â”œâ”€â”€ whisper/           # Whisper Tiny (Audio Encoder)
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ pytorch_model.bin
â”œâ”€â”€ dwpose/            # Body pose estimation models
â”œâ”€â”€ face-parse-bisent/ # Face Parsing models
â”œâ”€â”€ GFPGANv1.4.pth     # For enhance.py
â””â”€â”€ resnet18-5c106cde.pth

```

---

Important folder: MuseTalk/MuseTalk/scripts:

Based on the code provided in the `MuseTalk/scripts` directory, here is an explanation of what each script does:

### 1. `inference.py` 


This is the main script for **offline inference**, used to generate a lip-synced video from a source video and an audio file.

* **Functionality:** It processes input videos by extracting frames, detecting faces, and encoding them. It then uses the MuseTalk model (UNet and VAE) to generate lip movements synchronized with the input audio.
* **Key Features:**
* **Configuration:** Reads tasks (pairs of audio and video) from a YAML configuration file (e.g., `test_img.yaml`).
* **Face Parsing:** Uses a face parsing model to blend the generated mouth region back into the original face seamlessly.
* **Optimization:** Supports `float16` precision to speed up inference.
* **Output:** Combines the generated visual frames with the input audio using `ffmpeg` to produce the final MP4 file.



### 2. `realtime_inference.py` --} to cache

This script simulates a **real-time inference** scenario designed for lower latency, suitable for digital avatars or chatbots.

* **Functionality:** Unlike `inference.py`, which processes everything from scratch, this script relies on **pre-computed "avatar" data**. It caches the VAE latent representations of the background video so that during inference, the model only needs to run the generation step (UNet) and decoding, skipping the costly encoding step.
* **Key Features:**
* **Avatar Class:** Manages a specific video avatar's state, loading cached latents (`latents.pt`) and coordinates (`coords.pkl`).
* **Preparation Mode:** Can automatically generate the necessary cache files if they don't exist.
* **Concurrency:** Uses Python `threading` and queues to process frame resizing and blending in parallel with the model's generation loop to maximize throughput.



### 3. `force_prep.py` 

This is a utility script dedicated to **manually generating the cache** required for real-time inference.

* **Functionality:** It performs the "Preparation Phase" of `realtime_inference.py` in isolation. It extracts frames, detects landmarks, and pre-calculates the VAE latents (the heavy computation) for a specific video.
* **Purpose:** It is useful for setting up an avatar (e.g., `bank_avatar_1`) beforehand so that the real-time server or inference script can start immediately without a setup delay. It saves the cache files (`latents.pt`, `masks.pt`, `mask_coords.pt`) to the results directory.

### 4. `preprocess.py`

This script is for **training data preparation**, not for generating videos. It processes raw video datasets to train the MuseTalk models.

* **Functionality:** It standardizes raw videos into a format suitable for training.
* **Key Steps:**
1. **Conversion:** Converts input videos to a fixed frame rate (25 FPS).
2. **Segmentation:** Splits long videos into shorter clips (e.g., 30 seconds).
3. **Metadata Extraction:** Uses `mmpose` and `FaceAlignment` to detect face bounding boxes and landmarks for every frame, saving the data to JSON files.
4. **Audio Extraction:** Separates audio tracks into `.wav` files.
5. **List Generation:** automatically splits data into training and validation lists based on the configuration.



### Bonus: `warm_api.py`

Although not explicitly requested, this file is present in the folder. It appears to be an **advanced inference script** focused on high-stability output.

* **Functionality:** It implements advanced filtering (One-Euro filters) to smooth out jittery landmarks and uses sophisticated blending techniques (Poisson blending, sub-pixel shifting) to improve the visual quality of the final composite video.



## 5. Phase 1: The Force-Prep Protocol (Caching)

The **Force-Prep** phase is the secret sauce of the Warm Server. It moves the heavy lifting from "Runtime" to "Setup Time."

### Theory of Operation

The script `scripts/force_prep.py` performs the following atomic operations:

1. **Frame Extraction:** Converts the source MP4 into a sequence of PNGs (stored in memory or temp disk).
2. **Face Alignment:** Uses `face_alignment` to detect 68 landmarks. It calculates the bounding box for the face.
3. **Coordinate Caching:** Saves the bounding box coordinates `(x1, y1, x2, y2)` to a pickle file (`coords.pkl`). This ensures the "camera" doesn't jitter during live inference because the coordinates are fixed.
4. **VAE Encoding:** It crops the face, normalizes it to `-1...1`, and passes it through the VAE Encoder. The result is a latent tensor of shape `(4, 32, 32)`.
5. **Latent Caching:** These tensors are concatenated and saved as `latents.pt`.

### Execution Guide

1. **Prepare Source Video:** Place your high-quality avatar video (e.g., `avatar_1.mp4`) in `data/video/`.
3. **Edit Config:** Open `scripts/force_prep.py`.
```python
AVATAR_ID = "my_avatar_v1"
VIDEO_PATH = "data/video/avatar_1.mp4"
BBOX_SHIFT = -5  # Adjusts the chin/mouth crop region

```


3. **Run Script:**
```bash
python -m scripts.force_prep

```


*Result:* A new folder `results/avatars/my_avatar_v1/` is created containing `latents.pt`, `coords.pkl`, etc.

------------------------

However, after this, you may notice you are encountering this error:


no file path: ./models/dwpose/dw-ll_ucoco_384.pth

this is because you haven't downloaded the weight of the open sourced model. the way to fix it is to paste this into terminal:

------------------------------------------------------------------------
python -c '
import os
from huggingface_hub import hf_hub_download

# Define the weights to download
downloads = [
    ("TMElyralab/MuseTalk", "musetalk/musetalk.json", "models/musetalk"),
    ("TMElyralab/MuseTalk", "musetalk/pytorch_model.bin", "models/musetalk"),
    ("TMElyralab/MuseTalk", "musetalkV15/musetalk.json", "models/musetalkV15"),
    ("TMElyralab/MuseTalk", "musetalkV15/unet.pth", "models/musetalkV15"),
    ("stabilityai/sd-vae-ft-mse", "config.json", "models/sd-vae"),
    ("stabilityai/sd-vae-ft-mse", "diffusion_pytorch_model.bin", "models/sd-vae"),
    ("openai/whisper-tiny", "config.json", "models/whisper"),
    ("openai/whisper-tiny", "pytorch_model.bin", "models/whisper"),
    ("openai/whisper-tiny", "preprocessor_config.json", "models/whisper"),
    ("yzd-v/DWPose", "dw-ll_ucoco_384.pth", "models/dwpose"),
    ("ByteDance/LatentSync", "latentsync_syncnet.pt", "models/syncnet")
]

print("Starting download...")
for repo, filename, local_dir in downloads:
    print(f"Downloading {filename}...")
    os.makedirs(local_dir, exist_ok=True)
    hf_hub_download(repo_id=repo, filename=filename, local_dir=local_dir)


# Download ResNet manually (not on HF Hub)
import urllib.request
print("Downloading resnet18...")
os.makedirs("models/face-parse-bisent", exist_ok=True)
urllib.request.urlretrieve(
    "https://download.pytorch.org/models/resnet18-5c106cde.pth", 
    "models/face-parse-bisent/resnet18-5c106cde.pth"
)
print("All downloads complete.")



---------------------------------------------------------------------------

If this still returns some other errors like this:

<img width="934" height="338" alt="Screenshot 2026-02-04 at 4 44 48â€¯pm" src="https://github.com/user-attachments/assets/cb7677b2-bee6-4fd1-9ed9-18c0759e5d2f" />

apply this:

---------------------------------------------------------------------------

python -c '
import os
import shutil
from huggingface_hub import hf_hub_download

def fix_or_download(repo, filename, expected_path, correct_local_dir_arg):
    # 1. Check if file exists in the "wrong" nested location
    # Previous script likely made: models/musetalkV15/musetalkV15/unet.pth
    nested_path = os.path.join(os.path.dirname(expected_path), filename)
    
    if os.path.exists(nested_path):
        print(f"Found nested file at {nested_path}. Moving to {expected_path}...")
        os.rename(nested_path, expected_path)
        # Try to remove empty nested dir
        try:
            os.rmdir(os.path.dirname(nested_path))
        except:
            pass
    elif os.path.exists(expected_path):
        print(f"âœ… File already exists at {expected_path}")
    else:
        print(f"File missing. Downloading {filename} to {expected_path}...")
        # To get models/musetalkV15/unet.pth, we set local_dir="models" 
        # because filename already contains "musetalkV15/"
        hf_hub_download(repo_id=repo, filename=filename, local_dir=correct_local_dir_arg)

# Fix V1.5 UNet
fix_or_download(
    "TMElyralab/MuseTalk", 
    "musetalkV15/unet.pth", 
    "models/musetalkV15/unet.pth",
    "models" 
)

# Fix V1.5 Config
fix_or_download(
    "TMElyralab/MuseTalk", 
    "musetalkV15/musetalk.json", 
    "models/musetalkV15/musetalk.json",
    "models"
)

print("Fix complete. Running prep...")
'
------------------------------------------------------------------------------------



## 6. Phase 2: High-Fidelity Enhancement

If your source video is 1080p or 4K, the standard MuseTalk output (256x256 face crop) might look soft. Use `enhance.py` to fix this.

### GFPGAN Integration

The `enhance.py` script wraps the **GFPGAN** (Generative Facial Prior GAN) framework. It acts as a restoration filter that "hallucinates" high-frequency details (pores, eyelashes, sharper teeth) onto the generated face.

### The Enhancement Workflow

You have two strategies here:

1. **Pre-Enhancement (Recommended):** Run `enhance.py` on your *source video* before running `force_prep.py`. This ensures the background and non-moving parts of the face are already HD.
2. **Post-Enhancement:** Run `enhance.py` on the final `full_session.mp4` after the chat is over.

**Usage:**
Edit `enhance.py` configuration:

```python
INPUT_VIDEO = "results/full_session.mp4"
OUTPUT_VIDEO = "results/full_session_hd.mp4"

```

Run the script:

```bash
python enhance.py

```

*Note:* The script extracts frames to a `temp_frames` directory, applies `restorer.enhance()`, and then restitches them using FFmpeg to preserve the audio sync.

Before:
<img width="504" height="760" alt="Screenshot 2026-02-04 at 4 46 32â€¯pm" src="https://github.com/user-attachments/assets/bcc47610-78dd-4953-9d42-ff95bb14b478" />

After:
<img width="539" height="950" alt="Screenshot 2026-02-04 at 4 45 33â€¯pm" src="https://github.com/user-attachments/assets/3bdfa7b4-1104-4597-a030-cdf7269415f1" />

---

## 7. Phase 3: The Warm Server (Runtime) (the main file to keep the render stable and effective) IMPORTANT

This is the core of the project. The `server_fast.py` script initializes the `warm_api.py` engine.

### Under the Hood: `warm_api.py`

The `RealTimeInference` class inside `warm_api.py` is a marvel of engineering designed for stability.

#### 1. One-Euro Filtering

Raw face detection is noisy. One frame the box is at `x=100`, the next at `x=101`. This causes the face to shake.
The **One-Euro Filter** (implemented in `OneEuro` class) is a low-pass filter that dynamically adjusts its aggressiveness based on speed.

* **Stationary Face:** High filtering (jitter removal).
* **Moving Head:** Low filtering (low latency tracking).

#### 2. Scale Locking

`warm_api.py` includes a `scale_lock_enabled` flag.

* **Warmup:** For the first 50 frames, it collects width/height statistics.
* **Lock:** It calculates the median size and **locks** the zoom level. This prevents the "pulsing head" effect where the face grows/shrinks slightly as the mouth opens.

#### 3. Affine Tracking & Sub-Pixel Shift

To handle head rotation (roll), the API uses `cv2.calcOpticalFlowPyrLK` (Lucas-Kanade) to track stable features (nose bridge, eyes).
It computes an **Affine Matrix** (`M_affine`) representing the rotation difference between the current frame and the reference. This matrix is applied to the generated mouth patch *before* pasting, ensuring the mouth rotates perfectly with the head.

You can add more features and algorithms in to help with the rendering stability. 

### API Endpoints (`server_fast.py`)

* **`POST /speak`**
* **Payload:** `{"text": "Hello world"}`
* **Action:**
1. Triggers `VoiceEngine` (TTS) to generate `output_raw.wav`.
2. Converts audio to 16kHz via FFmpeg.
3. `warm_model.run()` generates the visual clip.
4. `append_to_session()` stitches the clip to `full_session.mp4`.


* **Return:** JSON with the path to the updated session video.


* **`GET /reset`**
* **Action:** Deletes `full_session.mp4` to start a fresh conversation.



---

## 8. Phase 4: The Full-Stack Web Application (still need to work on, especially the frontend)

Located in `webapp/`, this is a fully functional chat interface.

### Backend Job System (`webapp/backend/main.py`)

Because video generation takes time (even if it's fast), we cannot block the HTTP request.

1. **Request:** User hits `/chat`.
2. **Job Creation:** Backend creates a UUID `job_id` and spawns a **Thread**.
3. **Thread Execution:**
* Call LLM (Simulated or Real).
* Call TTS (`_generate_tts_wav`).
* Call `render_avatar_video`.


4. **Status:** The main thread returns the `job_id` immediately.

### Frontend Polling Architecture (`webapp/frontend/app.js`)

The JavaScript frontend does not wait for a long HTTP response.

1. **Poll:** It checks `/status/{job_id}` every 500ms.
2. **Video Swap:** When status is `done`, it updates the `<video>` src attribute.
3. **Auto-Play Handling:** It manages the `video.play()` promise to avoid browser "User Activation" errors.

---

## 9. Configuration Reference

The behavior of the Warm Server is governed by `configs/inference/realtime_live.yaml`.

| Parameter | Type | Description | Recommended |
| --- | --- | --- | --- |
| `avatar_id` | String | The folder name in `results/avatars/`. Must match `force_prep`. | `bank_avatar_1` |
| `video_path` | Path | Source video file. | `data/video/bank.mp4` |
| `bbox_shift` | Int | Vertical offset for the mouth mask. | `-5` (More chin) |
| `batch_size` | Int | How many frames to process in parallel on GPU. | `4` (RTX 3060), `16` (RTX 4090) |
| `preparation` | Bool | Whether to run face detection at runtime. | **`False`** (Use Cache) |
| `fps` | Int | Target output FPS. | `25` |

### Warm API Internal Toggles

Inside `scripts/warm_api.py`, `RealTimeInference.__init__`:

* `self.blend_mode = 'feather'` or `'poisson'`. Poisson is higher quality but slower (CPU intensive).
* `self.subpixel_enabled = True`. Keeps the face stable.
* `self.scale_lock_enabled = True`. Prevents pulsing.

---

## 10. Troubleshooting & Optimization

### Common Errors

**1. "Error: Input video not found or empty" in `enhance.py**`

* *Cause:* The `INPUT_VIDEO` path in `enhance.py` is incorrect or the video has 0 frames.
* *Fix:* Verify the path logic. Note that `enhance.py` defines `INPUT_VIDEO = "results/..."`. Ensure you are pointing to the correct file.

**2. "FFmpeg command not found"**

* *Cause:* Python's `subprocess` or `os.system` cannot see `ffmpeg`.
* *Fix:* Add FFmpeg to your System PATH variables. Restart your terminal/IDE.

**3. "Model returned None" in `server_fast.py**`

* *Cause:* The inference pipeline failed to generate frames. Usually because the audio file was empty or the `avatar_id` cache is missing.
* *Fix:* Run `force_prep.py` again. Check `logs/` for specific CUDA errors.

**4. Face Jitter / Vibration**

* *Optimization:* In `warm_api.py`, lower the `beta` value in `OneEuro` filter (e.g., `0.05`). This increases smoothing but adds slight latency trails.

**5. OOM (Out of Memory)**

* *Optimization:* Reduce `BATCH_SIZE` in `force_prep.py` and in `realtime_live.yaml`.


Manderin version ï¼ˆä¸­æ–‡ç‰ˆæœ¬ï¼‰: 

# MuseTalkï¼šWarm-Server ç‰ˆæœ¬ï¼ˆv1.5ï¼‰

**æƒå¨æ¶æ„æŒ‡å— & ä½¿ç”¨æ‰‹å†Œï¼ˆå¼€å‘è€…/ç ”ç©¶è€…/ç³»ç»Ÿæ¶æ„å¸ˆç‰ˆï¼‰**

**ç‰ˆæœ¬ï¼š** 1.5.0-Warm-Production
**ä½œè€…ï¼š** ä¸ºå®æ—¶æ•°å­—äººäº¤äº’å®šåˆ¶ï¼ˆReal-Time Digital Human Interactionï¼‰
**æ¶æ„ï¼š** æŒä¹…åœ¨çº¿æ¨ç†ï¼ˆPersistent Latent Inferenceï¼‰+ è¿ç»­ä¼šè¯ç®¡ç†ï¼ˆContinuous Session Managementï¼‰

---

ä½ è¿™ä»½æ–‡æ¡£æ˜¯ **MuseTalk Warm-Server Edition (v1.5 Custom)** çš„ **æœ€ç»ˆæŠ€æœ¯æ‰‹å†Œä¸æ–‡æ¡£**ã€‚å®ƒä¸ºå¸Œæœ›éƒ¨ç½² **äºšç§’çº§å»¶è¿Ÿ** çš„å®æ—¶æ•°å­—äººç³»ç»Ÿçš„å¼€å‘è€…ã€ç ”ç©¶äººå‘˜ä¸ç³»ç»Ÿæ¶æ„å¸ˆæä¾›å®Œæ•´æŒ‡å¯¼ã€‚

---

## âœ… é¦–å…ˆè¯·åœ¨ç»ˆç«¯æ‰§è¡Œï¼ˆå¾ˆé‡è¦ï¼‰

è¯·å…ˆ `cd MuseTalk/MuseTalk` å†æ‰§è¡Œåç»­å‘½ä»¤ï¼Œå¹¶å®‰è£… `requirements.txt`ã€‚

`requirements.txt` é‡ŒåŒ…å«è¿è¡Œè¯¥é¡¹ç›®çš„**æœ€æ–°å¯ç”¨ä¾èµ–**ã€‚ä½†ä½ è¿˜éœ€è¦é€šè¿‡å®‰è£… GitHub ä»“åº“ä¾èµ–ä¸ä¸‹è½½æƒé‡æ–‡ä»¶æ¥è¡¥å…¨è¿è¡Œç¯å¢ƒ â€”â€” **è¯·ç”¨ LLMï¼ˆå¤§æ¨¡å‹ï¼‰æ¥å¸®ä½ ç®¡ç†è¿™éƒ¨åˆ†**ï¼ˆå› ä¸ºä¾èµ–ä¼šç»å¸¸å˜åŠ¨ã€æ‰‹åŠ¨æŸ¥å¾ˆéº»çƒ¦ï¼‰ã€‚

> ï¼ˆä½ å¯èƒ½éœ€è¦èŠ±è¾ƒé•¿æ—¶é—´ä¸‹è½½ä¸æ›´æ–°ä¾èµ–ï¼‰

å¦å¤–ï¼šå³ä½¿ä½ å·²ç»ç”¨äº† Kalman / One-Euro ç­‰å¤šç§å»å™ªç¨³å®šç®—æ³•ï¼Œå˜´éƒ¨è¿åŠ¨ä»ç„¶æœ‰ç‚¹ä¸å¤ŸçœŸå®ã€‚

---

## ğŸ“š ç›®å½•

1. **æ‰§è¡Œæ‘˜è¦ & æ ¸å¿ƒç†å¿µ**
2. **ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ**

   * â€œå†·å¯åŠ¨ï¼ˆCold Startï¼‰â€é—®é¢˜
   * â€œWarm Serverï¼ˆçƒ­å¯åŠ¨æœåŠ¡ï¼‰â€è§£å†³æ–¹æ¡ˆ
   * è¿ç»­ä¼šè¯æµæ°´çº¿ï¼ˆContinuous Session Pipelineï¼‰
3. **ç¡¬ä»¶ & è½¯ä»¶å‰ç½®æ¡ä»¶**
4. **å®‰è£…ä¸ç¯å¢ƒé…ç½®**
5. **é˜¶æ®µ 1ï¼šForce-Prep åè®®ï¼ˆç¼“å­˜ï¼‰**

   * å·¥ä½œåŸç†
   * æ‰§è¡ŒæŒ‡å—
6. **é˜¶æ®µ 2ï¼šé«˜ä¿çœŸå¢å¼º**

   * GFPGAN é›†æˆ
   * å¢å¼ºå·¥ä½œæµ
7. **é˜¶æ®µ 3ï¼šWarm Serverï¼ˆè¿è¡Œæ—¶ï¼‰**

   * API ç«¯ç‚¹
   * `warm_api.py` çš„å†…éƒ¨æœºåˆ¶
   * ç¨³å®šç®—æ³•ï¼ˆOne-Euroã€ä»¿å°„ç­‰ï¼‰
8. **é˜¶æ®µ 4ï¼šå…¨æ ˆ Web åº”ç”¨**

   * åç«¯ Job ç³»ç»Ÿ
   * å‰ç«¯è½®è¯¢æ¶æ„
9. **é…ç½®å‚æ•°å‚è€ƒ**
10. **æ’é”™ä¸ä¼˜åŒ–**

---

## 1. æ‰§è¡Œæ‘˜è¦ & æ ¸å¿ƒç†å¿µ

æ ‡å‡†çš„ç”Ÿæˆå¼ AI è§†é¢‘æ¨¡å‹å®ç°ï¼ˆå°¤å…¶æ˜¯ **MuseTalk**ï¼‰å­˜åœ¨ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼š**å†·å¯åŠ¨å»¶è¿Ÿï¼ˆCold Start Latencyï¼‰**ã€‚

åœ¨æœ€æœ´ç´ çš„å®ç°ä¸­ï¼ˆnaive implementationï¼‰ï¼Œæ¯æ¬¡è¯·æ±‚ç”Ÿæˆå£å‹è§†é¢‘éƒ½ä¼šè§¦å‘ä¸€è¿ä¸²â€œé‡æ“ä½œâ€ï¼š

* å°† **4GB+ çš„ PyTorch æƒé‡**åŠ è½½è¿› VRAM
* å®ä¾‹åŒ– VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰å’Œ UNet
* ä»ç¡¬ç›˜è¯»å–æºè§†é¢‘
* å¯¹æ¯ä¸€å¸§éƒ½åšäººè„¸æ£€æµ‹ï¼ˆä¾‹å¦‚ S3FDï¼‰
* å°†å¸§ç¼–ç æˆ latent å¼ é‡

å› æ­¤ï¼Œå³ä¾¿ä½ åªæƒ³ç”Ÿæˆ **5 ç§’**çš„å›å¤è§†é¢‘ï¼Œ**å¯åŠ¨å¼€é”€**ä¹Ÿå¯èƒ½è¦ **10â€“15 ç§’** æ‰å¼€å§‹çœŸæ­£æ¨ç† â€”â€” è¿™è®©å®æ—¶å¯¹è¯å‡ ä¹ä¸å¯èƒ½ã€‚

**MuseTalk Warm-Server ç‰ˆæœ¬**ä»æ ¹æœ¬ä¸Šé‡æ„äº†è¿™ä¸ªæµç¨‹ï¼šæŠŠç³»ç»Ÿåšæˆ**æŒä¹…ã€å¯å¤ç”¨ã€çŠ¶æ€åŒ–æœåŠ¡**ã€‚é€šè¿‡å°†â€œå‡†å¤‡é˜¶æ®µï¼ˆPreparation Phaseï¼‰â€ä¸â€œæ¨ç†é˜¶æ®µï¼ˆInference Phaseï¼‰â€è§£è€¦ï¼Œæˆ‘ä»¬å®ç°ï¼š

1. **äºšç§’çº§é¦–å¸§æ—¶é—´ï¼ˆTTFFï¼ŒTime-to-First-Frameï¼‰**ï¼šæ¨¡å‹å¸¸é©» VRAMï¼ˆä¿æŒ warmï¼‰ï¼Œéšæ—¶å¯æ¥æ”¶éŸ³é¢‘å¼ é‡ã€‚
2. **é›¶å¼€é”€äººè„¸æ£€æµ‹ï¼ˆZero-Shot Face Detectionï¼‰**ï¼šäººè„¸åæ ‡ä¸èƒŒæ™¯ latents åœ¨ `force_prep.py` é¢„è®¡ç®—å¹¶ç¼“å­˜ï¼Œå®æ—¶æ¨ç†ä¸­ä¸å†åš CV è®¡ç®—ã€‚
3. **è¿ç»­å¯¹è¯è¾“å‡º**ï¼šä¸æ˜¯ç”Ÿæˆ `video_1.mp4`ã€`video_2.mp4` è¿™ç§ç¢ç‰‡æ–‡ä»¶ï¼Œè€Œæ˜¯é€šè¿‡ `server_fast.py` + FFmpeg æ‹¼æ¥æœºåˆ¶ï¼ŒæŠŠæ¯æ®µæ–°å¥å­æ— ç¼æ‹¼è¿›ä¸€ä¸ªâ€œæŒç»­å¢é•¿â€çš„ä¼šè¯è§†é¢‘é‡Œã€‚

---

## 2. ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ

### â€œå†·å¯åŠ¨ï¼ˆCold Startï¼‰â€é—®é¢˜

åœ¨æ ‡å‡†æ¨ç†è„šæœ¬ï¼ˆå¦‚åŸç‰ˆ `inference.py`ï¼‰ä¸­ï¼Œä¸€æ¬¡è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸé€šå¸¸æ˜¯ï¼š

1. `import torch`ï¼ˆ2sï¼‰
2. `åŠ è½½ UNet/VAE`ï¼ˆ4sï¼‰
3. `äººè„¸æ£€æµ‹`ï¼ˆæ¯å¸§ ~100msï¼‰
4. `VAE ç¼–ç `ï¼ˆæ¯å¸§ ~50msï¼‰
5. `UNet æ¨ç†`ï¼ˆå¿«ï¼‰
6. `VAE è§£ç `ï¼ˆå¿«ï¼‰
7. `æ¸…ç†èµ„æº`

å¯¹ä¸€ä¸ª **100 å¸§ï¼ˆçº¦ 4 ç§’ï¼‰** çš„è§†é¢‘æ¥è¯´ï¼Œä»…æ­¥éª¤ 3 ä¸ 4 å°±å¯èƒ½é¢å¤–å¢åŠ  **15 ç§’** çš„å»¶è¿Ÿï¼ˆåœ¨ä¸­æ¡£ GPU ä¸Šï¼‰ã€‚

å¦å¤–ï¼šå½“ä½ ç¬¬ä¸€æ¬¡æŠŠ AI å£å‹æ¸²æŸ“åˆ°ä¸Šä¼ è§†é¢‘ä¸Šæ—¶ï¼Œåœ¨ä½ç«¯ GPU ä¸Šå¯èƒ½éœ€è¦ **é•¿è¾¾ 10 åˆ†é’Ÿ**ã€‚ä½†æ¸²æŸ“å®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥â€¦â€¦

ï¼ˆæ­¤å¤„ä½ åŸæ–‡å¥å­æœªå†™å®Œï¼Œæˆ‘ä¿æŒåŸæ ·ä¸è¡¥å†™ã€‚ï¼‰

---

### â€œWarm Serverï¼ˆçƒ­å¯åŠ¨æœåŠ¡ï¼‰â€è§£å†³æ–¹æ¡ˆ

Warm Serverï¼ˆ`server_fast.py` + `warm_api.py`ï¼‰æŠŠç”Ÿå‘½å‘¨æœŸæ”¹æˆï¼š

1. **æœåŠ¡å™¨å¯åŠ¨æ—¶**ï¼šåŠ è½½æ‰€æœ‰æ¨¡å‹ä¸ç¼“å­˜ï¼ˆåªåšä¸€æ¬¡ï¼‰
2. **ç©ºé—²ç­‰å¾…**ï¼šå ç”¨ ~6GB VRAMï¼Œç­‰å¾…è¯·æ±‚
3. **æ”¶åˆ°è¯·æ±‚**ï¼š

   * éŸ³é¢‘ â†’ Whisper ç‰¹å¾æå–ï¼ˆ~100msï¼‰
   * UNet æ¨ç†ï¼ˆç›´æ¥èµ° GPUï¼‰
   * VAE è§£ç  + æ··åˆåˆæˆ
   * **æ€»å»¶è¿Ÿï¼š** é¦–æ¬¡å¼€å§‹ç”Ÿæˆå¤§çº¦ ~0.8s

---

### è¿ç»­ä¼šè¯æµæ°´çº¿ï¼ˆContinuous Session Pipelineï¼‰

ä¸ºäº†æ¨¡æ‹Ÿè§†é¢‘é€šè¯ï¼Œæˆ‘ä»¬ä¸èƒ½åªè¿”å› â€œvideo_1.mp4â€ï¼Œå†è¿”å› â€œvideo_2.mp4â€ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæŒç»­å¢é•¿çš„ã€å•ä¸€çš„è¾“å‡ºæµã€‚

* **ä¼šè¯æ–‡ä»¶ï¼š** `results/full_session.mp4`
* **æ‹¼æ¥å™¨ï¼š** æ¯æ¬¡ç”Ÿæˆæ–°ç‰‡æ®µåï¼Œ`server_fast.py` ä¼šç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡æœ¬åˆ—è¡¨ï¼ˆ`concat_list.txt`ï¼‰ï¼Œé‡Œé¢å†™çš„æ˜¯ *æ—§ä¼šè¯è§†é¢‘* ä¸ *æ–°ç‰‡æ®µ* çš„ç»å¯¹è·¯å¾„
* **FFmpeg æ‹¼æ¥ï¼š** æ‰§è¡Œï¼š

  `ffmpeg -f concat -c copy ...`

å…¶ä¸­ `-c copy` éå¸¸å…³é”®ï¼šå®ƒåšçš„æ˜¯**æ¯”ç‰¹æµæ‹·è´**ï¼ˆbitstream copyï¼‰ï¼Œä¸ä¼šé‡æ–°ç¼–ç å¸§ï¼Œåªæ›´æ–°å®¹å™¨å…ƒæ•°æ®ã€‚å› æ­¤æ‹¼æ¥å‡ ä¸ªå°æ—¶è§†é¢‘ä¹Ÿèƒ½åœ¨æ¯«ç§’çº§å®Œæˆã€‚

---

## 3. ç¡¬ä»¶ & è½¯ä»¶å‰ç½®æ¡ä»¶

### ç¡¬ä»¶

* **GPUï¼š** æ¨è NVIDIA RTX 3060ï¼ˆ12GBï¼‰æˆ–æ›´é«˜

  * **æœ€ä½ï¼š** 8GB VRAMï¼ˆå¦‚ RTX 2070ï¼‰ï¼Œå¯èƒ½éœ€è¦é™ä½ batch size
  * **ç†æƒ³ï¼š** RTX 4090ï¼ˆ24GBï¼‰ï¼Œå¯æ”¯æŒ batch 32+ ä¸ 4K ç¼“å­˜
* **å­˜å‚¨ï¼š** NVMe SSDï¼ˆå…³é”®ï¼‰

  * ä»æœºæ¢°ç¡¬ç›˜è¯»å–å¤§ `.pt` latent ç¼“å­˜ä¼šé€ æˆå¡é¡¿
* **å†…å­˜ï¼š** 32GB ç³»ç»Ÿå†…å­˜

### è½¯ä»¶

* **OSï¼š** Linuxï¼ˆUbuntu 20.04/22.04ï¼‰æˆ– Windows 10/11ï¼ˆPowerShell/CMDï¼‰
* **Pythonï¼š** 3.10.xï¼ˆä¸¥æ ¼è¦æ±‚ï¼‰
* **CUDAï¼š** 11.8 æˆ– 12.1
* **FFmpegï¼š** **å…³é”®ä¾èµ–**ï¼Œå¿…é¡»å®‰è£…å¹¶åŠ å…¥ PATH

  * é€šè¿‡è¿è¡Œ `ffmpeg -version` éªŒè¯

-----------------

## 4. å®‰è£…ä¸ç¯å¢ƒé…ç½®

### Step 1ï¼šå…‹éš†ä¸åˆ›å»ºç¯å¢ƒ

```bash
# Clone the repository
git clone https://github.com/TMElyralab/MuseTalk.git
cd MuseTalk/MuseTalk

# create and activate environment
python3 venv venv
source venv/bin/activate

# Install PyTorch (Ensure CUDA compatibility)
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
```

### Step 2ï¼šå®‰è£…æ ¸å¿ƒä¾èµ–

ä½¿ç”¨æä¾›çš„ `requirements.txt`ï¼š

```bash
pip install -r requirements.txt
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.1"
mim install "mmdet>=3.1.0"
mim install "mmpose>=1.1.0"
```

> æ³¨ï¼šå¢å¼ºæ¨¡å—è¿˜éœ€è¦ `gfpgan`

```bash
pip install gfpgan basicsr
```

---

## Step 3ï¼šæ¨¡å‹æƒé‡è·å–ï¼ˆModel Weight Acquisitionï¼‰

ä½ å¿…é¡»æŠŠ `models/` ç›®å½•ç»„ç»‡æˆå¦‚ä¸‹ç»“æ„ã€‚Linux å¯ç”¨ `download_weights.sh`ï¼ŒWindows éœ€è¦æ‰‹åŠ¨æ”¾ç½®æ–‡ä»¶ã€‚ï¼ˆç”±äºä¾èµ–ç»å¸¸å˜åŒ–ï¼Œå»ºè®®â€œé—® AI/LLM è·å–æœ€æ–°æ­£ç¡®æ–¹æ³•â€ï¼‰

**ç›®å½•ç»“æ„ï¼š**

```text
./models/
â”œâ”€â”€ musetalkV15/       # The V1.5 UNet Checkpoint
â”‚   â”œâ”€â”€ musetalk.json
â”‚   â””â”€â”€ unet.pth
â”œâ”€â”€ sd-vae/            # Stable Diffusion VAE (ft-mse)
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ diffusion_pytorch_model.bin
â”œâ”€â”€ whisper/           # Whisper Tiny (Audio Encoder)
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ pytorch_model.bin
â”œâ”€â”€ dwpose/            # Body pose estimation models
â”œâ”€â”€ face-parse-bisent/ # Face Parsing models
â”œâ”€â”€ GFPGANv1.4.pth     # For enhance.py
â””â”€â”€ resnet18-5c106cde.pth
```

----------------------------------
MuseTalk/MuseTalk/scripts: (æ¯”è¾ƒé‡è¦çš„æ–‡ä»¶å¤¹ï¼‰

### 1. `inference.py`

è¿™æ˜¯ç”¨äº **ç¦»çº¿æ¨ç† (Offline Inference)** çš„ä¸»è„šæœ¬ï¼Œç”¨äºæ ¹æ®æºè§†é¢‘å’ŒéŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå£å‹åŒæ­¥çš„è§†é¢‘ã€‚

* **åŠŸèƒ½ï¼š** å®ƒå¤„ç†è¾“å…¥è§†é¢‘ï¼Œæå–å¸§ï¼Œæ£€æµ‹äººè„¸å¹¶å¯¹å…¶è¿›è¡Œç¼–ç ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨ MuseTalk æ¨¡å‹ï¼ˆUNet å’Œ VAEï¼‰ç”Ÿæˆä¸è¾“å…¥éŸ³é¢‘åŒæ­¥çš„å”‡éƒ¨åŠ¨ä½œã€‚
* **ä¸»è¦ç‰¹ç‚¹ï¼š**
* **é…ç½®ï¼š** ä» YAML é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ `test_img.yaml`ï¼‰ä¸­è¯»å–ä»»åŠ¡ï¼ˆéŸ³é¢‘å’Œè§†é¢‘å¯¹ï¼‰ã€‚
* **é¢éƒ¨è§£æ (Face Parsing)ï¼š** ä½¿ç”¨é¢éƒ¨è§£ææ¨¡å‹å°†ç”Ÿæˆçš„å˜´éƒ¨åŒºåŸŸæ— ç¼èåˆå›åŸå§‹é¢éƒ¨ã€‚
* **ä¼˜åŒ–ï¼š** æ”¯æŒ `float16` ç²¾åº¦ä»¥åŠ é€Ÿæ¨ç†ã€‚
* **è¾“å‡ºï¼š** ä½¿ç”¨ `ffmpeg` å°†ç”Ÿæˆçš„è§†è§‰å¸§ä¸è¾“å…¥éŸ³é¢‘ç»“åˆï¼Œç”Ÿæˆæœ€ç»ˆçš„ MP4 æ–‡ä»¶ã€‚



### 2. `realtime_inference.py`

æ­¤è„šæœ¬æ¨¡æ‹Ÿ **å®æ—¶æ¨ç† (Real-time Inference)** åœºæ™¯ï¼Œä¸“ä¸ºä½å»¶è¿Ÿåº”ç”¨ï¼ˆå¦‚æ•°å­—å¤´åƒæˆ–èŠå¤©æœºå™¨äººï¼‰è®¾è®¡ã€‚

* **åŠŸèƒ½ï¼š** ä¸ä»å¤´å¼€å§‹å¤„ç†æ‰€æœ‰å†…å®¹çš„ `inference.py` ä¸åŒï¼Œæ­¤è„šæœ¬ä¾èµ–äº **é¢„å…ˆè®¡ç®—çš„â€œå¤´åƒâ€æ•°æ®**ã€‚å®ƒç¼“å­˜èƒŒæ™¯è§†é¢‘çš„ VAE æ½œåœ¨è¡¨ç¤ºï¼ˆLatentsï¼‰ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åªéœ€è¦è¿è¡Œç”Ÿæˆæ­¥éª¤ï¼ˆUNetï¼‰å’Œè§£ç ï¼Œä»è€Œè·³è¿‡äº†è€—æ—¶çš„ç¼–ç æ­¥éª¤ã€‚
* **ä¸»è¦ç‰¹ç‚¹ï¼š**
* **Avatar ç±»ï¼š** ç®¡ç†ç‰¹å®šè§†é¢‘å¤´åƒçš„çŠ¶æ€ï¼ŒåŠ è½½ç¼“å­˜çš„æ½œåœ¨è¡¨ç¤º (`latents.pt`) å’Œåæ ‡ (`coords.pkl`)ã€‚
* **å‡†å¤‡æ¨¡å¼ï¼š** å¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå®ƒä»¬ã€‚
* **å¹¶å‘ï¼š** ä½¿ç”¨ Python çš„ `threading` å’Œé˜Ÿåˆ—å¹¶è¡Œå¤„ç†å¸§çš„å¤§å°è°ƒæ•´å’Œæ··åˆï¼Œä¸æ¨¡å‹çš„ç”Ÿæˆå¾ªç¯åŒæ—¶è¿›è¡Œï¼Œä»¥æœ€å¤§åŒ–ååé‡ã€‚



### 3. `force_prep.py`

è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äº **æ‰‹åŠ¨ç”Ÿæˆå®æ—¶æ¨ç†æ‰€éœ€ç¼“å­˜** çš„å®ç”¨è„šæœ¬ã€‚

* **åŠŸèƒ½ï¼š** å®ƒç‹¬ç«‹æ‰§è¡Œ `realtime_inference.py` ä¸­çš„â€œå‡†å¤‡é˜¶æ®µâ€ã€‚å®ƒæå–å¸§ï¼Œæ£€æµ‹å…³é”®ç‚¹ï¼Œå¹¶ä¸ºç‰¹å®šè§†é¢‘é¢„å…ˆè®¡ç®— VAE æ½œåœ¨è¡¨ç¤ºï¼ˆè¿™æ˜¯ç¹é‡çš„è®¡ç®—éƒ¨åˆ†ï¼‰ã€‚
* **ç›®çš„ï¼š** ç”¨äºé¢„å…ˆè®¾ç½®å¤´åƒï¼ˆä¾‹å¦‚ `bank_avatar_1`ï¼‰ï¼Œä»¥ä¾¿å®æ—¶æœåŠ¡å™¨æˆ–æ¨ç†è„šæœ¬å¯ä»¥ç«‹å³å¯åŠ¨ï¼Œè€Œæ— éœ€è®¾ç½®å»¶è¿Ÿã€‚å®ƒå°†ç¼“å­˜æ–‡ä»¶ï¼ˆ`latents.pt`, `masks.pt`, `mask_coords.pt`ï¼‰ä¿å­˜åˆ°ç»“æœç›®å½•ä¸­ã€‚

### 4. `preprocess.py`

æ­¤è„šæœ¬ç”¨äº **è®­ç»ƒæ•°æ®å‡†å¤‡**ï¼Œè€Œä¸æ˜¯ç”¨äºç”Ÿæˆè§†é¢‘ã€‚å®ƒå¤„ç†åŸå§‹è§†é¢‘æ•°æ®é›†ä»¥è®­ç»ƒ MuseTalk æ¨¡å‹ã€‚

* **åŠŸèƒ½ï¼š** å°†åŸå§‹è§†é¢‘æ ‡å‡†åŒ–ä¸ºé€‚åˆè®­ç»ƒçš„æ ¼å¼ã€‚
* **ä¸»è¦æ­¥éª¤ï¼š**
1. **è½¬æ¢ï¼š** å°†è¾“å…¥è§†é¢‘è½¬æ¢ä¸ºå›ºå®šçš„å¸§ç‡ï¼ˆ25 FPSï¼‰ã€‚
2. **åˆ†å‰²ï¼š** å°†é•¿è§†é¢‘åˆ†å‰²æˆè¾ƒçŸ­çš„ç‰‡æ®µï¼ˆä¾‹å¦‚ 30 ç§’ï¼‰ã€‚
3. **å…ƒæ•°æ®æå–ï¼š** ä½¿ç”¨ `mmpose` å’Œ `FaceAlignment` æ£€æµ‹æ¯ä¸€å¸§çš„äººè„¸è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹ï¼Œå¹¶å°†æ•°æ®ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­ã€‚
4. **éŸ³é¢‘æå–ï¼š** å°†éŸ³è½¨åˆ†ç¦»ä¸º `.wav` æ–‡ä»¶ã€‚
5. **åˆ—è¡¨ç”Ÿæˆï¼š** æ ¹æ®é…ç½®è‡ªåŠ¨å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒåˆ—è¡¨å’ŒéªŒè¯åˆ—è¡¨ã€‚

--------------------------------------------------------------------------

## 5. é˜¶æ®µ 1ï¼šForce-Prep åè®®ï¼ˆç¼“å­˜ï¼‰

**Force-Prep** æ˜¯ Warm Server çš„â€œç§˜å¯†æ­¦å™¨â€ï¼šå®ƒæŠŠè¿è¡Œæ—¶çš„é‡è®¡ç®—è½¬ç§»åˆ°â€œéƒ¨ç½²å‡†å¤‡é˜¶æ®µâ€ã€‚

### å·¥ä½œåŸç†ï¼ˆTheory of Operationï¼‰

`scripts/force_prep.py` çš„åŸå­æ“ä½œæµç¨‹ï¼š

1. **æŠ½å¸§ï¼š** å°†æº MP4 è½¬ä¸º PNG å¸§åºåˆ—ï¼ˆå†…å­˜æˆ–ä¸´æ—¶ç£ç›˜ï¼‰
2. **äººè„¸å¯¹é½ï¼š** ç”¨ `face_alignment` æ£€æµ‹ 68 ä¸ªå…³é”®ç‚¹å¹¶è®¡ç®—äººè„¸ bbox
3. **åæ ‡ç¼“å­˜ï¼š** å°† bbox åæ ‡ `(x1, y1, x2, y2)` ä¿å­˜åˆ° `coords.pkl`

   * è¿™æ ·åæ ‡å›ºå®šï¼Œæ¨ç†æ—¶ä¸ä¼šâ€œæ‘„åƒæœºæŠ–åŠ¨â€
4. **VAE ç¼–ç ï¼š** è£å‰ªäººè„¸å¹¶å½’ä¸€åŒ–åˆ° `-1...1`ï¼Œé€å…¥ VAE Encoder

   * å¾—åˆ° `(4, 32, 32)` çš„ latent å¼ é‡
5. **Latent ç¼“å­˜ï¼š** å°†æ‰€æœ‰å¸§ latent æ‹¼æ¥å¹¶ä¿å­˜ä¸º `latents.pt`

### æ‰§è¡ŒæŒ‡å—ï¼ˆExecution Guideï¼‰

1. **å‡†å¤‡æºè§†é¢‘ï¼š** æŠŠé«˜è´¨é‡ avatar è§†é¢‘ï¼ˆå¦‚ `avatar_1.mp4`ï¼‰æ”¾åˆ° `data/video/`
2. **ç¼–è¾‘é…ç½®ï¼š** æ‰“å¼€ `scripts/force_prep.py`

```python
AVATAR_ID = "my_avatar_v1"
VIDEO_PATH = "data/video/avatar_1.mp4"
BBOX_SHIFT = -5  # Adjusts the chin/mouth crop region
```

3. **è¿è¡Œè„šæœ¬ï¼š**

```bash
python -m scripts.force_prep
```

**ç»“æœï¼š** ä¼šç”Ÿæˆ `results/avatars/my_avatar_v1/`ï¼Œå…¶ä¸­åŒ…å« `latents.pt`ã€`coords.pkl` ç­‰æ–‡ä»¶ã€‚

---

### å¸¸è§é”™è¯¯ï¼šç¼ºå°‘ dwpose æƒé‡

Force-prep ä¹‹åä½ å¯èƒ½ä¼šé‡åˆ°ï¼š

`no file path: ./models/dwpose/dw-ll_ucoco_384.pth`

åŸå› ï¼šä½ æ²¡ä¸‹è½½å¼€æºæ¨¡å‹çš„æƒé‡ã€‚ä¿®å¤æ–¹å¼ï¼šæŠŠä¸‹é¢å‘½ä»¤ç²˜è´´åˆ°ç»ˆç«¯è¿è¡Œï¼š

```bash
python -c '
import os
from huggingface_hub import hf_hub_download

# Define the weights to download
downloads = [
    ("TMElyralab/MuseTalk", "musetalk/musetalk.json", "models/musetalk"),
    ("TMElyralab/MuseTalk", "musetalk/pytorch_model.bin", "models/musetalk"),
    ("TMElyralab/MuseTalk", "musetalkV15/musetalk.json", "models/musetalkV15"),
    ("TMElyralab/MuseTalk", "musetalkV15/unet.pth", "models/musetalkV15"),
    ("stabilityai/sd-vae-ft-mse", "config.json", "models/sd-vae"),
    ("stabilityai/sd-vae-ft-mse", "diffusion_pytorch_model.bin", "models/sd-vae"),
    ("openai/whisper-tiny", "config.json", "models/whisper"),
    ("openai/whisper-tiny", "pytorch_model.bin", "models/whisper"),
    ("openai/whisper-tiny", "preprocessor_config.json", "models/whisper"),
    ("yzd-v/DWPose", "dw-ll_ucoco_384.pth", "models/dwpose"),
    ("ByteDance/LatentSync", "latentsync_syncnet.pt", "models/syncnet")
]

print("Starting download...")
for repo, filename, local_dir in downloads:
    print(f"Downloading {filename}...")
    os.makedirs(local_dir, exist_ok=True)
    hf_hub_download(repo_id=repo, filename=filename, local_dir=local_dir)


# Download ResNet manually (not on HF Hub)
import urllib.request
print("Downloading resnet18...")
os.makedirs("models/face-parse-bisent", exist_ok=True)
urllib.request.urlretrieve(
    "https://download.pytorch.org/models/resnet18-5c106cde.pth", 
    "models/face-parse-bisent/resnet18-5c106cde.pth"
)
print("All downloads complete.")
'
```

---

### å¦‚æœä½ åˆé‡åˆ°åµŒå¥—è·¯å¾„é”™è¯¯ï¼ˆä¾‹å¦‚æˆªå›¾æ‰€ç¤ºï¼‰

å¦‚æœä»ç„¶æŠ¥å‡ºç±»ä¼¼é”™è¯¯ï¼ˆä½ çš„æˆªå›¾åœºæ™¯ï¼‰ï¼Œè¯·ç”¨ä¸‹é¢è„šæœ¬ä¿®å¤ï¼š

```bash
python -c '
import os
import shutil
from huggingface_hub import hf_hub_download

def fix_or_download(repo, filename, expected_path, correct_local_dir_arg):
    # 1. Check if file exists in the "wrong" nested location
    # Previous script likely made: models/musetalkV15/musetalkV15/unet.pth
    nested_path = os.path.join(os.path.dirname(expected_path), filename)
    
    if os.path.exists(nested_path):
        print(f"Found nested file at {nested_path}. Moving to {expected_path}...")
        os.rename(nested_path, expected_path)
        # Try to remove empty nested dir
        try:
            os.rmdir(os.path.dirname(nested_path))
        except:
            pass
    elif os.path.exists(expected_path):
        print(f"âœ… File already exists at {expected_path}")
    else:
        print(f"File missing. Downloading {filename} to {expected_path}...")
        # To get models/musetalkV15/unet.pth, we set local_dir="models" 
        # because filename already contains "musetalkV15/"
        hf_hub_download(repo_id=repo, filename=filename, local_dir=correct_local_dir_arg)

# Fix V1.5 UNet
fix_or_download(
    "TMElyralab/MuseTalk", 
    "musetalkV15/unet.pth", 
    "models/musetalkV15/unet.pth",
    "models" 
)

# Fix V1.5 Config
fix_or_download(
    "TMElyralab/MuseTalk", 
    "musetalkV15/musetalk.json", 
    "models/musetalkV15/musetalk.json",
    "models"
)

print("Fix complete. Running prep...")
'
```

---

## 6. é˜¶æ®µ 2ï¼šé«˜ä¿çœŸå¢å¼ºï¼ˆHigh-Fidelity Enhancementï¼‰

å¦‚æœä½ çš„æºè§†é¢‘æ˜¯ 1080p æˆ– 4Kï¼Œæ ‡å‡† MuseTalk è¾“å‡ºï¼ˆ256x256 äººè„¸è£å‰ªï¼‰å¯èƒ½ä¼šåè½¯ã€‚å¯ä»¥ç”¨ `enhance.py` æ”¹å–„æ¸…æ™°åº¦ã€‚

### GFPGAN é›†æˆ

`enhance.py` å°è£…äº† **GFPGAN**ï¼ˆGenerative Facial Prior GANï¼‰ä½œä¸ºä¿®å¤æ»¤é•œï¼Œå®ƒä¼šâ€œè„‘è¡¥â€é«˜é¢‘ç»†èŠ‚ï¼ˆæ¯›å­”ã€ç«æ¯›ã€æ›´æ¸…æ™°çš„ç‰™é½¿ç­‰ï¼‰ï¼Œè®©è„¸æ›´é”åˆ©ã€‚

### å¢å¼ºå·¥ä½œæµï¼ˆEnhancement Workflowï¼‰

ä¸¤ç§ç­–ç•¥ï¼š

1. **é¢„å¢å¼ºï¼ˆæ¨èï¼‰ï¼š** åœ¨è·‘ `force_prep.py` ä¹‹å‰å…ˆå¯¹æºè§†é¢‘è·‘ `enhance.py`

   * è¿™æ ·èƒŒæ™¯ä¸é™æ€åŒºåŸŸå…ˆå˜ HDï¼Œåç»­å£å‹åˆæˆæ•´ä½“æ›´ç¨³
2. **åå¢å¼ºï¼š** ç­‰å¯¹è¯ç»“æŸåï¼Œå¯¹æœ€ç»ˆ `full_session.mp4` å†è·‘å¢å¼º

**ç”¨æ³•ï¼š** ä¿®æ”¹ `enhance.py` é…ç½®ï¼š

```python
INPUT_VIDEO = "results/full_session.mp4"
OUTPUT_VIDEO = "results/full_session_hd.mp4"
```

è¿è¡Œï¼š

```bash
python enhance.py
```

> è„šæœ¬ä¼šæŠ½å¸§åˆ° `temp_frames`ï¼Œå¯¹æ¯å¸§æ‰§è¡Œ `restorer.enhance()`ï¼Œå†ç”¨ FFmpeg æ‹¼å›è§†é¢‘ä»¥ä¿æŒéŸ³ç”»åŒæ­¥ã€‚

ï¼ˆä½ æä¾›çš„ Before/After å›¾ç‰‡ä¿æŒä¸å˜ï¼Œè¿™é‡Œä¸ç¿»è¯‘å›¾ç‰‡å†…å®¹ã€‚ï¼‰

---

## 7. é˜¶æ®µ 3ï¼šWarm Serverï¼ˆè¿è¡Œæ—¶ï¼‰â€”â€” ä¸»æ–‡ä»¶ï¼ˆéå¸¸é‡è¦ï¼‰

è¿™æ˜¯é¡¹ç›®æ ¸å¿ƒã€‚`server_fast.py` ä¼šåˆå§‹åŒ– `warm_api.py` å¼•æ“ã€‚

### `warm_api.py` å†…éƒ¨æœºåˆ¶ï¼ˆUnder the Hoodï¼‰

`warm_api.py` ä¸­çš„ `RealTimeInference` ç±»ä¸“ä¸ºç¨³å®šæ€§è€Œè®¾è®¡ã€‚

#### 1ï¼‰One-Euro æ»¤æ³¢

åŸå§‹äººè„¸æ£€æµ‹æœ‰å™ªå£°ã€‚ä¸Šä¸€å¸§ bbox å¯èƒ½ `x=100`ï¼Œä¸‹ä¸€å¸§ `x=101`ï¼Œå¯¼è‡´ç”»é¢è½»å¾®æŠ–åŠ¨ã€‚
**One-Euro Filter**ï¼ˆåœ¨ `OneEuro` ç±»é‡Œï¼‰æ˜¯ä¸€ç§åŠ¨æ€ä½é€šæ»¤æ³¢ï¼š

* **è„¸ä¸åŠ¨ï¼š** å¼ºæ»¤æ³¢ï¼ˆå»æŠ–åŠ¨ï¼‰
* **å¤´ç§»åŠ¨ï¼š** å¼±æ»¤æ³¢ï¼ˆå‡å»¶è¿Ÿï¼Œè·Ÿè¸ªæ›´çµæ•ï¼‰

#### 2ï¼‰å°ºåº¦é”å®šï¼ˆScale Lockingï¼‰

`warm_api.py` é‡Œæœ‰ `scale_lock_enabled`ï¼š

* **çƒ­èº«é˜¶æ®µï¼š** å‰ 50 å¸§ç»Ÿè®¡å®½é«˜
* **é”å®šï¼š** å–ä¸­ä½æ•°å¹¶é”ä½ç¼©æ”¾æ¯”ä¾‹ï¼Œé¿å…â€œå¤´ä¸€ä¼šå„¿å¤§ä¸€ä¼šå„¿å°â€çš„è„‰åŠ¨æ•ˆæœï¼ˆå˜´å¼ åˆæ—¶å°¤å…¶æ˜æ˜¾ï¼‰

#### 3ï¼‰ä»¿å°„è·Ÿè¸ª & äºšåƒç´ ä½ç§»ï¼ˆAffine + Sub-pixelï¼‰

ä¸ºäº†å¤„ç†å¤´éƒ¨æ—‹è½¬ï¼ˆrollï¼‰ï¼ŒAPI ä½¿ç”¨ `cv2.calcOpticalFlowPyrLK`ï¼ˆLucas-Kanade å…‰æµï¼‰è·Ÿè¸ªç¨³å®šç‰¹å¾ç‚¹ï¼ˆé¼»æ¢ã€çœ¼ç›ï¼‰ã€‚
å®ƒè®¡ç®—ä¸€ä¸ª **ä»¿å°„çŸ©é˜µ** `M_affine` è¡¨ç¤ºå½“å‰å¸§ç›¸å¯¹å‚è€ƒå¸§çš„æ—‹è½¬å·®å¼‚ï¼Œå¹¶åœ¨è´´ mouth patch ä¹‹å‰å…ˆæŠŠç”Ÿæˆç»“æœåšä»¿å°„å˜æ¢ï¼Œä½¿å˜´éƒ¨èƒ½ä¸å¤´éƒ¨æ—‹è½¬åŒæ­¥ã€‚

> ä½ è¿˜å¯ä»¥ç»§ç»­åŠ å…¥æ›´å¤šç‰¹å¾ä¸ç®—æ³•æ¥æå‡æ¸²æŸ“ç¨³å®šæ€§ã€‚

---

### API ç«¯ç‚¹ï¼ˆ`server_fast.py`ï¼‰

#### `POST /speak`

* **Payloadï¼š** `{"text": "Hello world"}`

* **æµç¨‹ï¼š**

  1. è§¦å‘ `VoiceEngine`ï¼ˆTTSï¼‰ç”Ÿæˆ `output_raw.wav`
  2. ç”¨ FFmpeg è½¬ 16kHz éŸ³é¢‘
  3. `warm_model.run()` ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
  4. `append_to_session()` æ‹¼æ¥è¿› `full_session.mp4`

* **è¿”å›ï¼š** JSONï¼ˆåŒ…å«æ›´æ–°åçš„ä¼šè¯è§†é¢‘è·¯å¾„ï¼‰

#### `GET /reset`

* **ä½œç”¨ï¼š** åˆ é™¤ `full_session.mp4` ä»¥å¼€å§‹æ–°ä¼šè¯

---

## 8. é˜¶æ®µ 4ï¼šå…¨æ ˆ Web åº”ç”¨ï¼ˆä»éœ€å®Œå–„ï¼Œå°¤å…¶å‰ç«¯ï¼‰

ä½äº `webapp/`ï¼Œæä¾›å¯ç”¨çš„èŠå¤© UIã€‚

### åç«¯ Job ç³»ç»Ÿï¼ˆ`webapp/backend/main.py`ï¼‰

å› ä¸ºè§†é¢‘ç”Ÿæˆä»éœ€è¦æ—¶é—´ï¼ˆå³ä½¿å¾ˆå¿«ï¼‰ï¼Œä¸èƒ½é˜»å¡ HTTP è¯·æ±‚ï¼š

1. ç”¨æˆ·è¯·æ±‚ `/chat`
2. åç«¯ç”Ÿæˆ UUID `job_id` å¹¶å¯åŠ¨ä¸€ä¸ª **Thread**
3. çº¿ç¨‹æ‰§è¡Œï¼š

   * è°ƒç”¨ LLMï¼ˆæ¨¡æ‹Ÿæˆ–çœŸå®ï¼‰
   * è°ƒç”¨ TTSï¼ˆ`_generate_tts_wav`ï¼‰
   * è°ƒç”¨ `render_avatar_video`
4. ä¸»çº¿ç¨‹ç«‹å³è¿”å› `job_id`

### å‰ç«¯è½®è¯¢æ¶æ„ï¼ˆ`webapp/frontend/app.js`ï¼‰

å‰ç«¯ä¸ç­‰å¾…é•¿è¯·æ±‚è¿”å›ï¼š

1. æ¯ 500ms è½®è¯¢ `/status/{job_id}`
2. å½“çŠ¶æ€ä¸º `done`ï¼Œæ›´æ–° `<video>` çš„ `src`
3. å¤„ç† `video.play()` Promiseï¼Œé¿å…æµè§ˆå™¨ â€œUser Activationâ€ æŠ¥é”™

---

## 9. é…ç½®å‚æ•°å‚è€ƒ

Warm Server çš„è¡Œä¸ºç”± `configs/inference/realtime_live.yaml` æ§åˆ¶ï¼š

| å‚æ•°            | ç±»å‹     | æè¿°                                          | æ¨è                             |
| ------------- | ------ | ------------------------------------------- | ------------------------------ |
| `avatar_id`   | String | `results/avatars/` ä¸‹çš„æ–‡ä»¶å¤¹åï¼Œå¿…é¡»åŒ¹é… `force_prep` | `bank_avatar_1`                |
| `video_path`  | Path   | æºè§†é¢‘è·¯å¾„                                       | `data/video/bank.mp4`          |
| `bbox_shift`  | Int    | mouth mask çš„å‚ç›´åç§»                            | `-5`ï¼ˆæ›´å¤šä¸‹å·´ï¼‰                     |
| `batch_size`  | Int    | GPU å¹¶è¡Œå¤„ç†å¸§æ•°                                  | RTX 3060 ç”¨ `4`ï¼ŒRTX 4090 ç”¨ `16` |
| `preparation` | Bool   | æ˜¯å¦è¿è¡Œæ—¶åšäººè„¸æ£€æµ‹                                  | **`False`**ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰              |
| `fps`         | Int    | è¾“å‡ºç›®æ ‡ FPS                                    | `25`                           |

### Warm API å†…éƒ¨å¼€å…³

åœ¨ `scripts/warm_api.py` çš„ `RealTimeInference.__init__`ï¼š

* `self.blend_mode = 'feather'` æˆ– `'poisson'`

  * Poisson è´¨é‡æ›´é«˜ä½†æ›´æ…¢ï¼ˆCPU é‡ï¼‰
* `self.subpixel_enabled = True`

  * è®©è„¸æ›´ç¨³å®š
* `self.scale_lock_enabled = True`

  * é˜²æ­¢ pulsingï¼ˆå¤´éƒ¨è½»å¾®ç¼©æ”¾ï¼‰

---

## 10. æ’é”™ä¸ä¼˜åŒ–ï¼ˆTroubleshooting & Optimizationï¼‰

### å¸¸è§é”™è¯¯

**1ï¼‰`enhance.py` æŠ¥ â€œError: Input video not found or emptyâ€**

* **åŸå› ï¼š** `enhance.py` é‡Œçš„ `INPUT_VIDEO` è·¯å¾„ä¸æ­£ç¡®æˆ–è§†é¢‘ 0 å¸§
* **ä¿®å¤ï¼š** æ£€æŸ¥è·¯å¾„ä¸æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼›æ³¨æ„ `enhance.py` é»˜è®¤ `INPUT_VIDEO = "results/..."`ï¼Œç¡®è®¤æŒ‡å‘æ­£ç¡®æ–‡ä»¶

**2ï¼‰â€œFFmpeg command not foundâ€**

* **åŸå› ï¼š** Python çš„ `subprocess` æˆ– `os.system` æ‰¾ä¸åˆ° `ffmpeg`
* **ä¿®å¤ï¼š** æŠŠ FFmpeg åŠ å…¥ç³»ç»Ÿ PATHï¼Œé‡å¯ç»ˆç«¯/IDE

**3ï¼‰`server_fast.py` æŠ¥ â€œModel returned Noneâ€**

* **åŸå› ï¼š** æ¨ç†æµæ°´çº¿æ²¡ç”Ÿæˆå¸§ï¼Œå¸¸è§æ˜¯éŸ³é¢‘ä¸ºç©ºæˆ– avatar ç¼“å­˜ç¼ºå¤±
* **ä¿®å¤ï¼š** é‡æ–°è·‘ `force_prep.py`ï¼Œå¹¶æ£€æŸ¥ `logs/` ä¸­æ˜¯å¦æœ‰ CUDA å…·ä½“é”™è¯¯

**4ï¼‰äººè„¸æŠ–åŠ¨ / éœ‡åŠ¨ï¼ˆFace Jitter / Vibrationï¼‰**

* **ä¼˜åŒ–ï¼š** åœ¨ `warm_api.py` ä¸­æŠŠ One-Euro çš„ `beta` è°ƒå°ï¼ˆå¦‚ `0.05`ï¼‰

  * å¹³æ»‘æ›´å¼ºï¼Œä½†ä¼šæœ‰è½»å¾®æ‹–å°¾ï¼ˆå¢åŠ ä¸€ç‚¹ç‚¹å»¶è¿Ÿæ„Ÿï¼‰

**5ï¼‰æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰**

* **ä¼˜åŒ–ï¼š** é™ä½ `force_prep.py` ä¸ `realtime_live.yaml` ä¸­çš„ `BATCH_SIZE`


